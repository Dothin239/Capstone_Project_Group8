{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Libraries üìö‚¨á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-05T01:54:35.238002Z",
     "iopub.status.busy": "2025-03-05T01:54:35.237635Z",
     "iopub.status.idle": "2025-03-05T01:54:35.244467Z",
     "shell.execute_reply": "2025-03-05T01:54:35.243553Z",
     "shell.execute_reply.started": "2025-03-05T01:54:35.237974Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import albumentations as album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T01:50:39.475043Z",
     "iopub.status.busy": "2025-03-05T01:50:39.474832Z",
     "iopub.status.idle": "2025-03-05T01:54:00.301141Z",
     "shell.execute_reply": "2025-03-05T01:54:00.299785Z",
     "shell.execute_reply.started": "2025-03-05T01:50:39.475023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Read Data & Create train / valid splits üìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-05T01:54:00.301703Z",
     "iopub.status.idle": "2025-03-05T01:54:00.301994Z",
     "shell.execute_reply": "2025-03-05T01:54:00.301882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#DATA_DIR = '/kaggle/input/deepglobe-road-extraction-dataset'\n",
    "DATA_DIR = '/kaggle/input/dataset-new/Dataset/Satellite Road Extraction Dataset'\n",
    "\n",
    "metadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\n",
    "metadata_df = metadata_df[metadata_df['split']=='train']\n",
    "metadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\n",
    "metadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "metadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n",
    "# Shuffle DataFrame\n",
    "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create train / valid splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Perform 90/10 split for train / val\n",
    "valid_df = metadata_df.sample(frac=0.1, random_state=42)\n",
    "train_df = metadata_df.drop(valid_df.index)\n",
    "len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_dict = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\n",
    "# Get class names\n",
    "class_names = class_dict['name'].tolist()\n",
    "# Get class RGB values\n",
    "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "print('All dataset classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shortlist specific classes to segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "select_classes = ['background', 'road']\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Helper functions for viz. & one-hot encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    \n",
    "    n_images = len(images) #count number of images\n",
    "    plt.figure(figsize=(20,8)) #set the figure size 20 by 8 inches\n",
    "    for idx, (name, image) in enumerate(images.items()): #iterate through the images\n",
    "        plt.subplot(1, n_images, idx + 1) #create a subplot for each image\n",
    "        plt.xticks([]); #hide x ticks\n",
    "        plt.yticks([]) #hide y ticks\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20) #set the title of the image\n",
    "        plt.imshow(image) #show the image\n",
    "    plt.show() #display the plot\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values): #label is the image, label_values is the RGB values of the classes\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    \n",
    "    semantic_map = [] #create an empty list\n",
    "    for colour in label_values: #iterate through the RGB values of the classes\n",
    "        equality = np.equal(label, colour) #check if the pixel value is equal to the RGB value of the class\n",
    "        class_map = np.all(equality, axis = -1) #check if all the values in the array are True\n",
    "        semantic_map.append(class_map) #append the class map to the semantic map list\n",
    "    semantic_map = np.stack(semantic_map, axis=-1) #stack the semantic map list along the depth axis\n",
    "\n",
    "    return semantic_map #return the semantic map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image): #image is the one-hot encoded image\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.argmax(image, axis = -1) #get the class key of the one-hot encoded image\n",
    "    return x #return the class key\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values): #image is the class key, label_values is the RGB values of the classes\n",
    "   \n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    colour_codes = np.array(label_values) #get the RGB values of the classes\n",
    "    x = colour_codes[image.astype(int)] #get the RGB values of the classes based on the class key\n",
    "\n",
    "    return x #return the RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RoadsDataset(torch.utils.data.Dataset):                   #create a custom dataset class\n",
    "\n",
    "    \"\"\"DeepGlobe Road Extraction Challenge Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        df (str): DataFrame containing images / labels paths\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(                                               #initialize the class\n",
    "            self, \n",
    "            df,\n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            target_size=(1024, 1024)\n",
    "    ):\n",
    "        self.image_paths = df['sat_image_path'].tolist()        #get the image paths\n",
    "        self.mask_paths = df['mask_path'].tolist()              #get the mask paths\n",
    "        \n",
    "        self.class_rgb_values = class_rgb_values                #get the RGB values of the classes\n",
    "        self.augmentation = augmentation                        #get the augmentation\n",
    "        self.preprocessing = preprocessing                      #get the preprocessing\n",
    "        self.target_size = target_size                          #get the target size\n",
    "    \n",
    "    def __getitem__(self, i):                                   #get the item at index i\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:                                   #if augmentation is not None\n",
    "            sample = self.augmentation(image=image, mask=mask)  #apply the augmentation\n",
    "            image, mask = sample['image'], sample['mask']       #get the augmented image and mask\n",
    "\n",
    "        image = cv2.resize(image, self.target_size)             #resize the image\n",
    "        mask = cv2.resize(mask, self.target_size)               #resize the mask\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing: #if preprocessing is not None\n",
    "            sample = self.preprocessing(image=image, mask=mask) #apply the preprocessing\n",
    "            image, mask = sample['image'], sample['mask']       #get the preprocessed image and mask\n",
    "\n",
    "        \n",
    "        return image, mask                                      #return the image and mask\n",
    "        \n",
    "    def __len__(self):                                          #return the length of the dataset\n",
    "        # return length of\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Sample Image and Mask üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = RoadsDataset(train_df, class_rgb_values=select_class_rgb_values)  #create the dataset\n",
    "random_idx = random.randint(0, len(dataset)-1)                              #get a random index\n",
    "image, mask = dataset[2]                                                    #get the image and mask at the random index\n",
    "\n",
    "visualize(                                                                  #visualize the image and mask\n",
    "    original_image = image,                                                 #original image\n",
    "    #ground truth mask\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)                            #one-hot encoded mask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining Augmentations üôÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():                    #create a function to get the training augmentation\n",
    "    train_transform = [                             #create a list of augmentations\n",
    "        album.HorizontalFlip(p=0.5),                #apply horizontal flip with a probability of 0.5\n",
    "        album.VerticalFlip(p=0.5),                  #apply vertical flip with a probability of 0.5\n",
    "    ]\n",
    "    return album.Compose(train_transform)           #return the augmentation\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):                         #create a function to convert the image and mask to tensor\n",
    "    return x.transpose(2, 0, 1).astype('float32')   #return the image and mask as a tensor\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):       #create a function to get the preprocessing transform\n",
    "\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = []                                                     #create an empty list\n",
    "    if preprocessing_fn:                                                #if preprocessing function is not None\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))         #append the preprocessing function to the list\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))    #append the to_tensor function to the list\n",
    "        \n",
    "    return album.Compose(_transform)                                    #return the list as a Compose object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Augmented Images & Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmented_dataset = RoadsDataset(               #create an augmented dataset\n",
    "    train_df,                                   #training dataframe\n",
    "    augmentation=get_training_augmentation(),   #training augmentation\n",
    "    class_rgb_values=select_class_rgb_values,   #RGB values of the classes\n",
    ")\n",
    "\n",
    "random_idx = random.randint(0, len(augmented_dataset)-1) #get a random index\n",
    "\n",
    "# Different augmentations on image/mask pairs\n",
    "for idx in range(3):                                            #iterate through the range 3\n",
    "    image, mask = augmented_dataset[idx]                        #get the image and mask at the index\n",
    "    visualize(                                                  #visualize the image and mask\n",
    "        original_image = image,                                 #original image\n",
    "        #ground truth mask\n",
    "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask),select_class_rgb_values),\n",
    "        one_hot_encoded_mask = reverse_one_hot(mask)            #one-hot encoded mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Training DeepLabV3+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'            #encoder name\n",
    "ENCODER_WEIGHTS = 'imagenet'    #encoder weights\n",
    "CLASSES = select_classes        #classes\n",
    "ACTIVATION = 'sigmoid'          #activation name\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3Plus(              #create a DeepLabV3Plus model\n",
    "    encoder_name=ENCODER,               #encoder name\n",
    "    encoder_weights=ENCODER_WEIGHTS,    #encoder weights\n",
    "    classes=len(CLASSES),               #number of classes\n",
    "    activation=ACTIVATION,              #activation function\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS) #get the preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Train / Val DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = RoadsDataset(                           #create the training dataset\n",
    "    train_df,                                           #training dataframe\n",
    "    augmentation=get_training_augmentation(),           #training augmentation\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),  #preprocessing\n",
    "    class_rgb_values=select_class_rgb_values,           #RGB values of the classes\n",
    ")\n",
    "\n",
    "valid_dataset = RoadsDataset(                           #create the validation dataset\n",
    "    valid_df,                                           #validation dataframe\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),  #preprocessing\n",
    "    class_rgb_values=select_class_rgb_values,           #RGB values of the classes\n",
    ")\n",
    "\n",
    "# Get train and val data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4) #create the training data loader\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=4) #create the validation data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Model Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch.utils as smp_utils #import segmentation models pytorch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 3\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [                                 #create a list of metrics\n",
    "    smp.utils.metrics.IoU(threshold=0.5),   #IoU\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([                      #create an Adam optimizer\n",
    "    dict(params=model.parameters(), lr=0.00008),    #set the learning rate\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( \n",
    "#    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    "#)\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth'): #if the model checkpoint exists\n",
    "    model = torch.load('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth', map_location=DEVICE) #load the model checkpoint\n",
    "    print('Loaded pre-trained DeepLabV3+ model!') #print a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define epoch parameters\n",
    "train_epoch = smp.utils.train.TrainEpoch(   #create a training epoch\n",
    "    model,                                  #model\n",
    "    loss=loss,                              #loss function\n",
    "    metrics=metrics,                        #metrics\n",
    "    optimizer=optimizer,                    #optimizer\n",
    "    device=DEVICE,                          #device\n",
    "    verbose=True,                           #verbose\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(   #create a validation epoch\n",
    "    model,                                  #model\n",
    "    loss=loss,                              #loss function\n",
    "    metrics=metrics,                        #metrics\n",
    "    device=DEVICE,                          #device\n",
    "    verbose=True,                           #verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training DeepLabV3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#measure the time taken to train the model\n",
    "%%time\n",
    "\n",
    "if TRAINING:                                                #if TRAINING is True\n",
    "\n",
    "    best_iou_score = 0.0                                    #initialize the best IoU score\n",
    "    train_logs_list, valid_logs_list = [], []               #create empty lists to store the training and validation logs\n",
    "\n",
    "    for i in range(0, EPOCHS):                              #iterate through the number of epochs\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))                      #print the epoch number\n",
    "        train_logs = train_epoch.run(train_loader)          #run the training epoch\n",
    "        valid_logs = valid_epoch.run(valid_loader)          #run the validation epoch\n",
    "        train_logs_list.append(train_logs)                  #append the training logs to the list\n",
    "        valid_logs_list.append(valid_logs)                  #append the validation logs to the list\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:        #if the best IoU score is less than the validation IoU score\n",
    "            best_iou_score = valid_logs['iou_score']        #update the best IoU score\n",
    "            torch.save(model, './best_model.pth')           #save the model checkpoint\n",
    "            print('Model saved!')                           #print a message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Prediction on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load best saved model checkpoint from the current run\n",
    "if os.path.exists('./best_model.pth'):                                  #if the model checkpoint exists\n",
    "    best_model = torch.load('./best_model.pth', map_location=DEVICE)    #load the model checkpoint\n",
    "    print('Loaded DeepLabV3+ model from this run.')                     #print a message\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "elif os.path.exists('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth'): #if the model checkpoint exists\n",
    "    best_model = torch.load('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth', map_location=DEVICE) #load the model checkpoint\n",
    "    print('Loaded DeepLabV3+ model from a previous commit.') #print a message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Sample Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create test dataloader to be used with DeepLabV3+ model (with preprocessing operation: to_tensor(...))\n",
    "test_dataset = RoadsDataset(                                #create the test dataset\n",
    "    valid_df,                                               #validation dataframe\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),      #preprocessing\n",
    "    class_rgb_values=select_class_rgb_values,               #RGB values of the classes\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)                  #create the test data loader\n",
    "\n",
    "# test dataset for visualization (without preprocessing augmentations & transformations)\n",
    "test_dataset_vis = RoadsDataset(                            #create the test dataset for visualization\n",
    "    valid_df,                                               #validation dataframe\n",
    "    class_rgb_values=select_class_rgb_values,               #RGB values of the classes\n",
    ")\n",
    "\n",
    "# get a random test image/mask index\n",
    "random_idx = random.randint(0, len(test_dataset_vis)-1)     #get a random index\n",
    "image, mask = test_dataset_vis[random_idx]                  #get the image and mask at the random index\n",
    "\n",
    "visualize(                                                  #visualize the image and mask\n",
    "    original_image = image,                                 #original image\n",
    "    #ground truth mask\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)            #one-hot encoded mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample Prediction Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_preds_folder = 'sample_predictions/' #create a folder to store the sample predictions\n",
    "if not os.path.exists(sample_preds_folder): #if the folder does not exist\n",
    "    os.makedirs(sample_preds_folder)        #create the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test & Predict Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(test_dataset)):                                #iterate through the test dataset\n",
    "\n",
    "    image, gt_mask = test_dataset[idx]                              #get the image and mask at the index\n",
    "    image_vis = test_dataset_vis[idx][0].astype('uint8')            #get the image at the index and convert it to uint8\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)      #convert the image to a tensor and move it to the device\n",
    "    # Predict test image\n",
    "    pred_mask = best_model(x_tensor)                                #get the predicted mask\n",
    "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()          #detach the mask, squeeze it and convert it to a numpy array\n",
    "    # Convert pred_mask from `CHW` format to `HWC` format\n",
    "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "    # Get prediction channel corresponding to foreground\n",
    "    pred_road_heatmap = pred_mask[:,:,select_classes.index('road')]  #get the prediction channel corresponding to the road class\n",
    "    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)     #colour code the segmentation mask\n",
    "    # Convert gt_mask from `CHW` format to `HWC` format\n",
    "    gt_mask = np.transpose(gt_mask,(1,2,0))                          #transpose the ground truth mask\n",
    "    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)   #colour code the ground truth mask\n",
    "    #save the image, ground truth mask and predicted mask\n",
    "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"),np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
    "    \n",
    "    visualize(                                                       #visualize the image, ground truth mask and predicted mask\n",
    "        original_image = image_vis,                                  #original image\n",
    "        ground_truth_mask = gt_mask,                                 #ground truth mask\n",
    "        predicted_mask = pred_mask,                                  #predicted mask\n",
    "        pred_road_heatmap = pred_road_heatmap,                       #predicted road heatmap\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_logs = test_epoch.run(test_dataloader)\n",
    "print(\"Evaluation on Test Data: \")\n",
    "print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n",
    "print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot Dice Loss for Train vs. Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_logs_df = pd.DataFrame(train_logs_list)\n",
    "valid_logs_df = pd.DataFrame(valid_logs_list)\n",
    "train_logs_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('IoU Score', fontsize=20)\n",
    "plt.title('IoU Score Plot', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('iou_score_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Dice Loss', fontsize=20)\n",
    "plt.title('Dice Loss Plot', fontsize=20)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('dice_loss_plot.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 966140,
     "sourceId": 1634186,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
