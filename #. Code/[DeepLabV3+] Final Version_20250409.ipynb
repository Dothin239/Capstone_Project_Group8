{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1634186,"sourceType":"datasetVersion","datasetId":966140}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A. Libraries","metadata":{}},{"cell_type":"code","source":"import os, cv2\nimport numpy as np\nimport pandas as pd\nimport random, tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport albumentations as album","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-10T20:30:08.066351Z","iopub.execute_input":"2025-04-10T20:30:08.066538Z","iopub.status.idle":"2025-04-10T20:30:14.298486Z","shell.execute_reply.started":"2025-04-10T20:30:08.066518Z","shell.execute_reply":"2025-04-10T20:30:14.297837Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:14.299253Z","iopub.execute_input":"2025-04-10T20:30:14.299767Z","iopub.status.idle":"2025-04-10T20:30:20.365729Z","shell.execute_reply.started":"2025-04-10T20:30:14.299735Z","shell.execute_reply":"2025-04-10T20:30:20.364908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2025-04-10T20:30:20.366746Z","iopub.execute_input":"2025-04-10T20:30:20.367069Z","iopub.status.idle":"2025-04-10T20:30:35.602022Z","shell.execute_reply.started":"2025-04-10T20:30:20.367035Z","shell.execute_reply":"2025-04-10T20:30:35.601269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# B. Read Data & Create train / validation splits","metadata":{}},{"cell_type":"markdown","source":"## 1. Read Data","metadata":{}},{"cell_type":"code","source":"#DATA_DIR = '/kaggle/input/deepglobe-road-extraction-dataset'\nDATA_DIR = '/kaggle/input/deepglobe-road-extraction-dataset'\n\nmetadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\nmetadata_df = metadata_df[metadata_df['split']=='train']\nmetadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\nmetadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\nmetadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n# Shuffle DataFrame\nmetadata_df = metadata_df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2025-04-10T20:30:35.602909Z","iopub.execute_input":"2025-04-10T20:30:35.603194Z","iopub.status.idle":"2025-04-10T20:30:35.684984Z","shell.execute_reply.started":"2025-04-10T20:30:35.603163Z","shell.execute_reply":"2025-04-10T20:30:35.684305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Create train / valid splits","metadata":{}},{"cell_type":"code","source":"# Perform 90/10 split for train / val\nvalid_df = metadata_df.sample(frac=0.1, random_state=42)\ntrain_df = metadata_df.drop(valid_df.index)\nlen(train_df), len(valid_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:35.685835Z","iopub.execute_input":"2025-04-10T20:30:35.686157Z","iopub.status.idle":"2025-04-10T20:30:35.694328Z","shell.execute_reply.started":"2025-04-10T20:30:35.686126Z","shell.execute_reply":"2025-04-10T20:30:35.693673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_dict = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\n# Get class names\nclass_names = class_dict['name'].tolist()\n# Get class RGB values\nclass_rgb_values = class_dict[['r','g','b']].values.tolist()\n\nprint('All dataset classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:35.695168Z","iopub.execute_input":"2025-04-10T20:30:35.695436Z","iopub.status.idle":"2025-04-10T20:30:35.717891Z","shell.execute_reply.started":"2025-04-10T20:30:35.695416Z","shell.execute_reply":"2025-04-10T20:30:35.717264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Shortlist specific classes to segment","metadata":{}},{"cell_type":"code","source":"# Useful to shortlist specific classes in datasets with large number of classes\nselect_classes = ['background', 'road']\n\n# Get RGB values of required classes\nselect_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\nselect_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n\nprint('Selected classes and their corresponding RGB values in labels:')\nprint('Class Names: ', class_names)\nprint('Class RGB values: ', class_rgb_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:35.720408Z","iopub.execute_input":"2025-04-10T20:30:35.720610Z","iopub.status.idle":"2025-04-10T20:30:35.726014Z","shell.execute_reply.started":"2025-04-10T20:30:35.720591Z","shell.execute_reply":"2025-04-10T20:30:35.725224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# C. Helper functions for viz. & one-hot encoding/decoding","metadata":{}},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \n    \"\"\"\n    Plot images in one row\n    \"\"\"\n    \n    n_images = len(images) #count number of images\n    plt.figure(figsize=(20,8)) #set the figure size 20 by 8 inches\n    for idx, (name, image) in enumerate(images.items()): #iterate through the images\n        plt.subplot(1, n_images, idx + 1) #create a subplot for each image\n        plt.xticks([]); #hide x ticks\n        plt.yticks([]) #hide y ticks\n        # get title from the parameter names\n        plt.title(name.replace('_',' ').title(), fontsize=20) #set the title of the image\n        plt.imshow(image) #show the image\n    plt.show() #display the plot\n\n# Perform one hot encoding on label\ndef one_hot_encode(label, label_values): #label is the image, label_values is the RGB values of the classes\n    \n    \"\"\"\n    Convert a segmentation image label array to one-hot format\n    by replacing each pixel value with a vector of length num_classes\n    # Arguments\n        label: The 2D array segmentation image label\n        label_values\n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of num_classes\n    \"\"\"\n    \n    semantic_map = [] #create an empty list\n    for colour in label_values: #iterate through the RGB values of the classes\n        equality = np.equal(label, colour) #check if the pixel value is equal to the RGB value of the class\n        class_map = np.all(equality, axis = -1) #check if all the values in the array are True\n        semantic_map.append(class_map) #append the class map to the semantic map list\n    semantic_map = np.stack(semantic_map, axis=-1) #stack the semantic map list along the depth axis\n\n    return semantic_map #return the semantic map\n    \n# Perform reverse one-hot-encoding on labels / preds\ndef reverse_one_hot(image): #image is the one-hot encoded image\n    \n    \"\"\"\n    Transform a 2D array in one-hot format (depth is num_classes),\n    to a 2D array with only 1 channel, where each pixel value is\n    the classified class key.\n    # Arguments\n        image: The one-hot format image \n    # Returns\n        A 2D array with the same width and hieght as the input, but\n        with a depth size of 1, where each pixel value is the classified \n        class key.\n    \"\"\"\n    \n    x = np.argmax(image, axis = -1) #get the class key of the one-hot encoded image\n    return x #return the class key\n\n# Perform colour coding on the reverse-one-hot outputs\ndef colour_code_segmentation(image, label_values): #image is the class key, label_values is the RGB values of the classes\n   \n    \"\"\"\n    Given a 1-channel array of class keys, colour code the segmentation results.\n    # Arguments\n        image: single channel array where each value represents the class key.\n        label_values\n    # Returns\n        Colour coded image for segmentation visualization\n    \"\"\"\n    \n    colour_codes = np.array(label_values) #get the RGB values of the classes\n    x = colour_codes[image.astype(int)] #get the RGB values of the classes based on the class key\n\n    return x #return the RGB image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:35.727645Z","iopub.execute_input":"2025-04-10T20:30:35.727842Z","iopub.status.idle":"2025-04-10T20:30:35.744083Z","shell.execute_reply.started":"2025-04-10T20:30:35.727820Z","shell.execute_reply":"2025-04-10T20:30:35.743236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RoadsDataset(torch.utils.data.Dataset):\n    \"\"\"\n    DeepGlobe Road Extraction Challenge Dataset. Read images, apply augmentation and preprocessing transformations.\n\n    Args:\n        df (DataFrame): DataFrame containing image and mask paths.\n        class_rgb_values (list): RGB values of select classes to extract from segmentation mask.\n        augmentation (albumentations.Compose): Data augmentation pipeline (e.g., flip, scale, etc.).\n        preprocessing (albumentations.Compose): Data preprocessing (e.g., normalization, shape manipulation, etc.).\n        target_size (tuple): Target size for resizing images and masks.\n        has_masks (bool): Whether the dataset includes masks.\n    \"\"\"\n\n    def __init__(self, df, class_rgb_values=None, augmentation=None, preprocessing=None, target_size=(1024, 1024), has_masks=True):\n        self.image_paths = df['sat_image_path'].tolist()  # Get the image paths\n        self.has_masks = has_masks  # Whether masks are available\n\n        if self.has_masks:\n            self.mask_paths = df['mask_path'].tolist()  # Get the mask paths\n\n        self.class_rgb_values = class_rgb_values  # RGB values of the classes\n        self.augmentation = augmentation  # Augmentation pipeline\n        self.preprocessing = preprocessing  # Preprocessing pipeline\n        self.target_size = target_size  # Target size for resizing\n\n    def __getitem__(self, i):\n        # Load the image\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, self.target_size)\n\n        if self.has_masks:\n            # Load the mask if available\n            mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n            mask = cv2.resize(mask, self.target_size)\n            mask = one_hot_encode(mask, self.class_rgb_values).astype('float')  # One-hot encode the mask\n\n            # Apply augmentation\n            if self.augmentation:\n                augmented = self.augmentation(image=image, mask=mask)\n                image, mask = augmented['image'], augmented['mask']\n\n            # Apply preprocessing\n            if self.preprocessing:\n                preprocessed = self.preprocessing(image=image, mask=mask)\n                image, mask = preprocessed['image'], preprocessed['mask']\n\n            return image, mask\n        else:\n            # Apply augmentation\n            if self.augmentation:\n                augmented = self.augmentation(image=image)\n                image = augmented['image']\n\n            # Apply preprocessing\n            if self.preprocessing:\n                preprocessed = self.preprocessing(image=image)\n                image = preprocessed['image']\n\n            return image\n\n    def __len__(self):\n        return len(self.image_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:35.744963Z","iopub.execute_input":"2025-04-10T20:30:35.745255Z","iopub.status.idle":"2025-04-10T20:30:35.762436Z","shell.execute_reply.started":"2025-04-10T20:30:35.745225Z","shell.execute_reply":"2025-04-10T20:30:35.761602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Visualize Sample Image and Mask","metadata":{}},{"cell_type":"code","source":"dataset = RoadsDataset(train_df, class_rgb_values=select_class_rgb_values)  #create the dataset\nrandom_idx = random.randint(0, len(dataset)-1)                              #get a random index\nimage, mask = dataset[2]                                                    #get the image and mask at the random index\n\nvisualize(                                                                  #visualize the image and mask\n    original_image = image,                                                 #original image\n    #ground truth mask\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)                            #one-hot encoded mask\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:35.763234Z","iopub.execute_input":"2025-04-10T20:30:35.763461Z","iopub.status.idle":"2025-04-10T20:30:36.862902Z","shell.execute_reply.started":"2025-04-10T20:30:35.763442Z","shell.execute_reply":"2025-04-10T20:30:36.862005Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Defining Augmentations","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():                    #create a function to get the training augmentation\n    train_transform = [                             #create a list of augmentations\n        album.HorizontalFlip(p=0.5),                #apply horizontal flip with a probability of 0.5\n        album.VerticalFlip(p=0.5),                  #apply vertical flip with a probability of 0.5\n    ]\n    return album.Compose(train_transform)           #return the augmentation\n\n\ndef to_tensor(x, **kwargs):                         #create a function to convert the image and mask to tensor\n    return x.transpose(2, 0, 1).astype('float32')   #return the image and mask as a tensor\n\n\ndef get_preprocessing(preprocessing_fn=None):       #create a function to get the preprocessing transform\n\n    \"\"\"Construct preprocessing transform    \n    Args:\n        preprocessing_fn (callable): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \"\"\"\n    \n    _transform = []                                                     #create an empty list\n    if preprocessing_fn:                                                #if preprocessing function is not None\n        _transform.append(album.Lambda(image=preprocessing_fn))         #append the preprocessing function to the list\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))    #append the to_tensor function to the list\n        \n    return album.Compose(_transform)                                    #return the list as a Compose object","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:36.863871Z","iopub.execute_input":"2025-04-10T20:30:36.864125Z","iopub.status.idle":"2025-04-10T20:30:36.869209Z","shell.execute_reply.started":"2025-04-10T20:30:36.864101Z","shell.execute_reply":"2025-04-10T20:30:36.868434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Visualize Augmented Images & Masks","metadata":{}},{"cell_type":"code","source":"augmented_dataset = RoadsDataset(               #create an augmented dataset\n    train_df,                                   #training dataframe\n    augmentation=get_training_augmentation(),   #training augmentation\n    class_rgb_values=select_class_rgb_values,   #RGB values of the classes\n)\n\nrandom_idx = random.randint(0, len(augmented_dataset)-1) #get a random index\n\n# Different augmentations on image/mask pairs\nfor idx in range(3):                                            #iterate through the range 3\n    image, mask = augmented_dataset[idx]                        #get the image and mask at the index\n    visualize(                                                  #visualize the image and mask\n        original_image = image,                                 #original image\n        #ground truth mask\n        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask),select_class_rgb_values),\n        one_hot_encoded_mask = reverse_one_hot(mask)            #one-hot encoded mask\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:36.870043Z","iopub.execute_input":"2025-04-10T20:30:36.870260Z","iopub.status.idle":"2025-04-10T20:30:38.980348Z","shell.execute_reply.started":"2025-04-10T20:30:36.870240Z","shell.execute_reply":"2025-04-10T20:30:38.979534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# D. Training DeepLabV3+","metadata":{}},{"cell_type":"markdown","source":"## 1. Model Definition","metadata":{}},{"cell_type":"code","source":"ENCODER = 'resnet50'            #encoder name\nENCODER_WEIGHTS = 'imagenet'    #encoder weights\nCLASSES = select_classes        #classes\nACTIVATION = 'sigmoid'          #activation name\n\n# create segmentation model with pretrained encoder\nmodel = smp.DeepLabV3Plus(              #create a DeepLabV3Plus model\n    encoder_name=ENCODER,               #encoder name\n    encoder_weights=ENCODER_WEIGHTS,    #encoder weights\n    classes=len(CLASSES),               #number of classes\n    activation=ACTIVATION,              #activation function\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS) #get the preprocessing function","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:38.981407Z","iopub.execute_input":"2025-04-10T20:30:38.981720Z","iopub.status.idle":"2025-04-10T20:30:39.966002Z","shell.execute_reply.started":"2025-04-10T20:30:38.981688Z","shell.execute_reply":"2025-04-10T20:30:39.965343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Get Train / Valid DataLoaders","metadata":{}},{"cell_type":"code","source":"# Get train and val dataset instances\ntrain_dataset = RoadsDataset(                           #create the training dataset\n    train_df,                                           #training dataframe\n    augmentation=get_training_augmentation(),           #training augmentation\n    preprocessing=get_preprocessing(preprocessing_fn),  #preprocessing\n    class_rgb_values=select_class_rgb_values,           #RGB values of the classes\n)\n\nvalid_dataset = RoadsDataset(                           #create the validation dataset\n    valid_df,                                           #validation dataframe\n    preprocessing=get_preprocessing(preprocessing_fn),  #preprocessing\n    class_rgb_values=select_class_rgb_values,           #RGB values of the classes\n)\n\n# Get train and val data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4) #create the training data loader\nvalid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=4) #create the validation data loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:39.966802Z","iopub.execute_input":"2025-04-10T20:30:39.967099Z","iopub.status.idle":"2025-04-10T20:30:39.974265Z","shell.execute_reply.started":"2025-04-10T20:30:39.967068Z","shell.execute_reply":"2025-04-10T20:30:39.973476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Set Model Hyperparams","metadata":{}},{"cell_type":"code","source":"import segmentation_models_pytorch.utils as smp_utils #import segmentation models pytorch utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:39.974933Z","iopub.execute_input":"2025-04-10T20:30:39.975115Z","iopub.status.idle":"2025-04-10T20:30:39.993646Z","shell.execute_reply.started":"2025-04-10T20:30:39.975098Z","shell.execute_reply":"2025-04-10T20:30:39.992830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\nTRAINING = True\n\n# Set num of epochs\nEPOCHS = 5\n\n# Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# define loss function\nloss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [                                 #create a list of metrics\n    smp.utils.metrics.IoU(threshold=0.5),   #IoU\n]\n\n# define optimizer\noptimizer = torch.optim.Adam([                      #create an Adam optimizer\n    dict(params=model.parameters(), lr=0.00008),    #set the learning rate\n])\n\n# define learning rate scheduler (not used in this NB)\n#lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts( \n#    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n#)\n\n# load best saved model checkpoint from previous commit (if present)\nif os.path.exists('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth'): #if the model checkpoint exists\n    model = torch.load('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth', map_location=DEVICE) #load the model checkpoint\n    print('Loaded pre-trained DeepLabV3+ model!') #print a message","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:39.994479Z","iopub.execute_input":"2025-04-10T20:30:39.994734Z","iopub.status.idle":"2025-04-10T20:30:40.054741Z","shell.execute_reply.started":"2025-04-10T20:30:39.994714Z","shell.execute_reply":"2025-04-10T20:30:40.053866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define epoch parameters\ntrain_epoch = smp.utils.train.TrainEpoch(   #create a training epoch\n    model,                                  #model\n    loss=loss,                              #loss function\n    metrics=metrics,                        #metrics\n    optimizer=optimizer,                    #optimizer\n    device=DEVICE,                          #device\n    verbose=True,                           #verbose\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(   #create a validation epoch\n    model,                                  #model\n    loss=loss,                              #loss function\n    metrics=metrics,                        #metrics\n    device=DEVICE,                          #device\n    verbose=True,                           #verbose\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:40.055577Z","iopub.execute_input":"2025-04-10T20:30:40.055906Z","iopub.status.idle":"2025-04-10T20:30:40.285467Z","shell.execute_reply.started":"2025-04-10T20:30:40.055875Z","shell.execute_reply":"2025-04-10T20:30:40.284564Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Training DeepLabV3+","metadata":{}},{"cell_type":"code","source":"%%time                                                      \n\nif TRAINING:                                                #if TRAINING is True\n\n    best_iou_score = 0.0                                    #initialize the best IoU score\n    train_logs_list, valid_logs_list = [], []               #create empty lists to store the training and validation logs\n\n    for i in range(0, EPOCHS):                              #iterate through the number of epochs\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))                      #print the epoch number\n        train_logs = train_epoch.run(train_loader)          #run the training epoch\n        valid_logs = valid_epoch.run(valid_loader)          #run the validation epoch\n        train_logs_list.append(train_logs)                  #append the training logs to the list\n        valid_logs_list.append(valid_logs)                  #append the validation logs to the list\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:        #if the best IoU score is less than the validation IoU score\n            best_iou_score = valid_logs['iou_score']        #update the best IoU score\n            torch.save(model, './best_model.pth')           #save the model checkpoint\n            print('Model saved!')                           #print a message","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T20:30:40.286234Z","iopub.execute_input":"2025-04-10T20:30:40.286475Z","iopub.status.idle":"2025-04-10T22:08:11.225859Z","shell.execute_reply.started":"2025-04-10T20:30:40.286456Z","shell.execute_reply":"2025-04-10T22:08:11.224987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# E. Prediction on Test Data in Training Folder","metadata":{}},{"cell_type":"markdown","source":"## 1. Load Trained Model","metadata":{}},{"cell_type":"code","source":"# load best saved model checkpoint from the current run\nif os.path.exists('./best_model.pth'):                                  #if the model checkpoint exists\n    best_model = torch.load('./best_model.pth', map_location=DEVICE)    #load the model checkpoint\n    print('Loaded DeepLabV3+ model from this run.')                     #print a message\n\n# load best saved model checkpoint from previous commit (if present)\nelif os.path.exists('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth'): #if the model checkpoint exists\n    t_model = torch.load('../input/road-extraction-from-satellite-images-deeplabv3/best_model.pth', map_location=DEVICE) #load the model checkpoint\n    print('Loaded DeepLabV3+ model from a previous commit.') #print a message","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:08:11.227033Z","iopub.execute_input":"2025-04-10T22:08:11.227331Z","iopub.status.idle":"2025-04-10T22:08:11.370033Z","shell.execute_reply.started":"2025-04-10T22:08:11.227285Z","shell.execute_reply":"2025-04-10T22:08:11.369342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Visualize Sample Validation Dataset in Training Folder","metadata":{}},{"cell_type":"code","source":"# create test dataloader to be used with DeepLabV3+ model (with preprocessing operation: to_tensor(...))\ntest_dataset = RoadsDataset(                                #create the test dataset\n    valid_df,                                               #validation dataframe\n    preprocessing=get_preprocessing(preprocessing_fn),      #preprocessing\n    class_rgb_values=select_class_rgb_values,               #RGB values of the classes\n)\n\ntest_dataloader = DataLoader(test_dataset)                  #create the test data loader\n\n# test dataset for visualization (without preprocessing augmentations & transformations)\ntest_dataset_vis = RoadsDataset(                            #create the test dataset for visualization\n    valid_df,                                               #validation dataframe\n    class_rgb_values=select_class_rgb_values,               #RGB values of the classes\n)\n\n# get a random test image/mask index\nrandom_idx = random.randint(0, len(test_dataset_vis)-1)     #get a random index\nimage, mask = test_dataset_vis[random_idx]                  #get the image and mask at the random index\n\nvisualize(                                                  #visualize the image and mask\n    original_image = image,                                 #original image\n    #ground truth mask\n    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n    one_hot_encoded_mask = reverse_one_hot(mask)            #one-hot encoded mask\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:08:11.370942Z","iopub.execute_input":"2025-04-10T22:08:11.371250Z","iopub.status.idle":"2025-04-10T22:08:12.098508Z","shell.execute_reply.started":"2025-04-10T22:08:11.371219Z","shell.execute_reply":"2025-04-10T22:08:12.097365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Create Sample Prediction Folder","metadata":{}},{"cell_type":"code","source":"sample_preds_folder = 'sample_predictions/' #create a folder to store the sample predictions\nif not os.path.exists(sample_preds_folder): #if the folder does not exist\n    os.makedirs(sample_preds_folder)        #create the folder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:08:12.099612Z","iopub.execute_input":"2025-04-10T22:08:12.099883Z","iopub.status.idle":"2025-04-10T22:08:12.103895Z","shell.execute_reply.started":"2025-04-10T22:08:12.099859Z","shell.execute_reply":"2025-04-10T22:08:12.103069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Test & Predict Validation Dataset in Training Folder","metadata":{}},{"cell_type":"code","source":"for idx in range(len(test_dataset)):                                #iterate through the test dataset\n\n    image, gt_mask = test_dataset[idx]                              #get the image and mask at the index\n    image_vis = test_dataset_vis[idx][0].astype('uint8')            #get the image at the index and convert it to uint8\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)      #convert the image to a tensor and move it to the device\n    # Predict test image\n    pred_mask = best_model(x_tensor)                                #get the predicted mask\n    pred_mask = pred_mask.detach().squeeze().cpu().numpy()          #detach the mask, squeeze it and convert it to a numpy array\n    # Convert pred_mask from `CHW` format to `HWC` format\n    pred_mask = np.transpose(pred_mask,(1,2,0))\n    # Get prediction channel corresponding to foreground\n    pred_road_heatmap = pred_mask[:,:,select_classes.index('road')]  #get the prediction channel corresponding to the road class\n    pred_mask = colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values)     #colour code the segmentation mask\n    # Convert gt_mask from `CHW` format to `HWC` format\n    gt_mask = np.transpose(gt_mask,(1,2,0))                          #transpose the ground truth mask\n    gt_mask = colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values)   #colour code the ground truth mask\n    #save the image, ground truth mask and predicted mask\n    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"),np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n    \n    #visualize(                                                       #visualize the image, ground truth mask and predicted mask\n        #original_image = image_vis,                                  #original image\n        #ground_truth_mask = gt_mask,                                 #ground truth mask\n        #predicted_mask = pred_mask,                                  #predicted mask\n        #pred_road_heatmap = pred_road_heatmap,                       #predicted road heatmap\n    #)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Evaluation on Validation Dataset in Training Folder","metadata":{}},{"cell_type":"code","source":"test_epoch = smp.utils.train.ValidEpoch(\n    model,\n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_logs = test_epoch.run(test_dataloader)\nprint(\"Evaluation on Test Data: \")\nprint(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\nprint(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:13:11.192427Z","iopub.execute_input":"2025-04-10T22:13:11.192647Z","iopub.status.idle":"2025-04-10T22:15:15.888222Z","shell.execute_reply.started":"2025-04-10T22:13:11.192628Z","shell.execute_reply":"2025-04-10T22:15:15.887388Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Plot Dice Loss/ IoU for Train vs. Val","metadata":{}},{"cell_type":"code","source":"train_logs_df = pd.DataFrame(train_logs_list)\nvalid_logs_df = pd.DataFrame(valid_logs_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:15:15.892163Z","iopub.execute_input":"2025-04-10T22:15:15.892447Z","iopub.status.idle":"2025-04-10T22:15:15.897555Z","shell.execute_reply.started":"2025-04-10T22:15:15.892424Z","shell.execute_reply":"2025-04-10T22:15:15.896895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_logs_df.T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:15:15.898772Z","iopub.execute_input":"2025-04-10T22:15:15.898965Z","iopub.status.idle":"2025-04-10T22:15:15.934850Z","shell.execute_reply.started":"2025-04-10T22:15:15.898947Z","shell.execute_reply":"2025-04-10T22:15:15.934240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_logs_df.T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:15:15.935556Z","iopub.execute_input":"2025-04-10T22:15:15.935748Z","iopub.status.idle":"2025-04-10T22:15:15.944382Z","shell.execute_reply.started":"2025-04-10T22:15:15.935731Z","shell.execute_reply":"2025-04-10T22:15:15.943600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('IoU Score', fontsize=20)\nplt.title('IoU Score Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('iou_score_plot.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:15:15.945056Z","iopub.execute_input":"2025-04-10T22:15:15.945233Z","iopub.status.idle":"2025-04-10T22:15:16.369355Z","shell.execute_reply.started":"2025-04-10T22:15:15.945216Z","shell.execute_reply":"2025-04-10T22:15:16.368527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\nplt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('Dice Loss', fontsize=20)\nplt.title('Dice Loss Plot', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('dice_loss_plot.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:15:16.370230Z","iopub.execute_input":"2025-04-10T22:15:16.370515Z","iopub.status.idle":"2025-04-10T22:15:16.800872Z","shell.execute_reply.started":"2025-04-10T22:15:16.370493Z","shell.execute_reply":"2025-04-10T22:15:16.799913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# F. Do Testing in Test Folder","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame for the Test Folder\n\n# Path to the test folder\nTEST_DIR = '/kaggle/input/deepglobe-road-extraction-dataset/test'\n\n# Create a DataFrame for the test images\ntest_metadata_df = pd.DataFrame({\n    'sat_image_path': [os.path.join(TEST_DIR, img) for img in os.listdir(TEST_DIR)]\n})\n\nprint(f\"Number of test images: {len(test_metadata_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:15:16.801654Z","iopub.execute_input":"2025-04-10T22:15:16.801878Z","iopub.status.idle":"2025-04-10T22:15:16.871505Z","shell.execute_reply.started":"2025-04-10T22:15:16.801849Z","shell.execute_reply":"2025-04-10T22:15:16.870697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the Test Dataset and DataLoader\n\n# Create the test dataset\ntest_dataset = RoadsDataset(\n    test_metadata_df,\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n    has_masks=False  # Specify that masks are not available\n)\n\n# Create the test DataLoader\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:15:16.872204Z","iopub.execute_input":"2025-04-10T22:15:16.872434Z","iopub.status.idle":"2025-04-10T22:15:16.876812Z","shell.execute_reply.started":"2025-04-10T22:15:16.872414Z","shell.execute_reply":"2025-04-10T22:15:16.876115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the trained model\ndef load_trained_model(model_path, device):\n    \"\"\"\n    Load the trained model from the specified path.\n    \"\"\"\n    model = torch.load(model_path, map_location=device)\n    model.eval()  # Set the model to evaluation mode\n    print(f\"Loaded model from {model_path}\")\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict masks for the test images\ndef predict_and_visualize_with_original(model, test_dataloader, test_metadata_df, num_images_to_test, class_rgb_values, device):\n    \"\"\"\n    Predict masks for the test images and visualize the results, including original images.\n\n    Args:\n        model: Trained model.\n        test_dataloader: DataLoader for the test dataset.\n        test_metadata_df: DataFrame containing metadata for test images (e.g., file paths).\n        num_images_to_test: Number of images to test.\n        class_rgb_values: RGB values for the classes.\n        device: Device to run the model on (e.g., 'cuda' or 'cpu').\n    \"\"\"\n    images_processed = 0\n\n    for idx, image_batch in enumerate(test_dataloader):\n        # Move the image batch to the device\n        image_batch = image_batch.to(device)\n\n        # Predict the mask\n        with torch.no_grad():\n            pred_masks = model(image_batch)\n            pred_masks = pred_masks.cpu().numpy()  # Move the predictions to CPU\n\n        # Process and visualize the predictions\n        for i in range(image_batch.shape[0]):\n            if images_processed >= num_images_to_test:\n                return  # Stop once the desired number of images is processed\n\n            # Load the original image (not preprocessed) for comparison\n            original_image_path = test_metadata_df.iloc[images_processed]['sat_image_path']\n            original_image = cv2.cvtColor(cv2.imread(original_image_path), cv2.COLOR_BGR2RGB)\n\n            # Preprocessed image (used for prediction)\n            preprocessed_image = image_batch[i].cpu().numpy().transpose(1, 2, 0)  # Convert CHW to HWC\n            preprocessed_image = np.clip(preprocessed_image * 255, 0, 255).astype('uint8')  # Scale back to 0–255\n\n            # Predicted mask\n            predicted_mask = pred_masks[i].transpose(1, 2, 0)  # Convert CHW to HWC\n            predicted_mask = colour_code_segmentation(reverse_one_hot(predicted_mask), class_rgb_values)\n\n            # Visualize the original image, preprocessed image, and predicted mask\n            plt.figure(figsize=(15, 5))\n            plt.subplot(1, 3, 1)\n            plt.title(\"Original Image (No Preprocessing)\")\n            plt.imshow(original_image)\n            plt.axis('off')\n\n            plt.subplot(1, 3, 2)\n            plt.title(\"Preprocessed Image\")\n            plt.imshow(preprocessed_image)\n            plt.axis('off')\n\n            plt.subplot(1, 3, 3)\n            plt.title(\"Predicted Mask\")\n            plt.imshow(predicted_mask)\n            plt.axis('off')\n\n            plt.show()\n\n            images_processed += 1\n\n# Example usage\nMODEL_PATH = './best_model.pth'  # Path to the trained model\nNUM_IMAGES_TO_TEST = 100  # Number of images to test\n\n# Load the trained model\nbest_model = load_trained_model(MODEL_PATH, DEVICE)\n\n# Predict and visualize with original images\npredict_and_visualize_with_original(best_model, test_dataloader, test_metadata_df, NUM_IMAGES_TO_TEST, select_class_rgb_values, DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:27:50.822964Z","iopub.execute_input":"2025-04-10T22:27:50.823286Z","iopub.status.idle":"2025-04-10T22:29:02.382759Z","shell.execute_reply.started":"2025-04-10T22:27:50.823261Z","shell.execute_reply":"2025-04-10T22:29:02.381920Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# G. Do Validation in Valid Folder (cloudy, blur, and noised images)","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame for the Valid Folder\n\n# Specify the file names you want to predict\nspecific_file_names = ['556202_sat.jpg', '551638_sat.jpg','524125_sat.jpg','386970_sat.jpg','382619_sat.jpg']  # Replace with your specific file names\n\n# Path to the valid folder\nVALID_DIR = '/kaggle/input/deepglobe-road-extraction-dataset/valid'\n\n# Create a DataFrame for the valid images\nvalid_metadata_df = pd.DataFrame({\n    'sat_image_path': [os.path.join(VALID_DIR, img) for img in os.listdir(VALID_DIR)]\n})\n\n# Filter the DataFrame for specific file names\nvalid_metadata_df = valid_metadata_df[valid_metadata_df['sat_image_path'].str.contains('|'.join(specific_file_names))]\n\nprint(f\"Number of selected valid images: {len(valid_metadata_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:16:06.633454Z","iopub.execute_input":"2025-04-10T22:16:06.633765Z","iopub.status.idle":"2025-04-10T22:16:06.693784Z","shell.execute_reply.started":"2025-04-10T22:16:06.633740Z","shell.execute_reply":"2025-04-10T22:16:06.692974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the Valid Dataset and DataLoader\n\n# Create the valid dataset\nvalid_dataset = RoadsDataset(\n    valid_metadata_df,\n    preprocessing=get_preprocessing(preprocessing_fn),\n    class_rgb_values=select_class_rgb_values,\n    has_masks=False  # Specify that masks are not available\n)\n\n# Create the valid DataLoader\nvalid_dataloader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:16:06.694697Z","iopub.execute_input":"2025-04-10T22:16:06.694946Z","iopub.status.idle":"2025-04-10T22:16:06.699162Z","shell.execute_reply.started":"2025-04-10T22:16:06.694926Z","shell.execute_reply":"2025-04-10T22:16:06.698392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform Predictions\n\n# Load the trained model\nbest_model = torch.load('./best_model.pth', map_location=DEVICE)\nbest_model.eval()  # Set the model to evaluation mode\n\n# Visualize predictions for valid images\nfor idx, image_batch in enumerate(valid_dataloader):\n    # Move the image batch to the device\n    image_batch = image_batch.to(DEVICE)\n\n    # Predict the mask\n    with torch.no_grad():\n        pred_masks = best_model(image_batch)\n        pred_masks = pred_masks.cpu().numpy()  # Move the predictions to CPU\n\n    # Process and visualize the predictions\n    for i in range(image_batch.shape[0]):\n        # Load the original image (not preprocessed) for comparison\n        original_image_path = valid_metadata_df.iloc[idx * valid_dataloader.batch_size + i]['sat_image_path']\n        original_image = cv2.cvtColor(cv2.imread(original_image_path), cv2.COLOR_BGR2RGB)\n\n        # Preprocessed image (used for prediction)\n        preprocessed_image = image_batch[i].cpu().numpy().transpose(1, 2, 0).astype('uint8')\n\n        # Predicted mask\n        predicted_mask = pred_masks[i].transpose(1, 2, 0)  # Convert CHW to HWC\n        predicted_mask = colour_code_segmentation(reverse_one_hot(predicted_mask), select_class_rgb_values)\n\n        # Visualize the original image, preprocessed image, and predicted mask\n        plt.figure(figsize=(15, 5))\n        plt.subplot(1, 3, 1)\n        plt.title(\"Original Image (No Preprocessing)\")\n        plt.imshow(original_image)\n        plt.axis('off')\n\n        plt.subplot(1, 3, 2)\n        plt.title(\"Preprocessed Image\")\n        plt.imshow(preprocessed_image)\n        plt.axis('off')\n\n        plt.subplot(1, 3, 3)\n        plt.title(\"Predicted Mask\")\n        plt.imshow(predicted_mask)\n        plt.axis('off')\n\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:16:06.700057Z","iopub.execute_input":"2025-04-10T22:16:06.700331Z","iopub.status.idle":"2025-04-10T22:16:11.004966Z","shell.execute_reply.started":"2025-04-10T22:16:06.700287Z","shell.execute_reply":"2025-04-10T22:16:11.004099Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# H. Compare the predicted masks from Training Folder and Test Folder","metadata":{}},{"cell_type":"code","source":"# Predict & Visualize images with ground truth and predictions during training\nsample_indices = [10, 20, 40, 100, 300]  # Modify sample indices as needed\nfor idx in sample_indices:\n    # Get preprocessed image and ground truth mask\n    image, mask = train_dataset[idx]\n\n    # Load the original image (not preprocessed) for comparison\n    original_image_path = train_df.iloc[idx]['sat_image_path']  # Get the original image path\n    original_image = cv2.cvtColor(cv2.imread(original_image_path), cv2.COLOR_BGR2RGB)  # Load original image\n\n    # Convert preprocessed image to tensor\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n\n    # Predict mask\n    with torch.no_grad():\n        pred_logits = model(x_tensor)\n    pred_np = pred_logits.squeeze().cpu().numpy()\n\n    # Convert to class indices and color-coded mask\n    pred_class_indices = np.argmax(pred_np, axis=0)\n    pred_colored = colour_code_segmentation(pred_class_indices, select_class_rgb_values)\n\n    # Convert ground truth mask to class indices if it's one-hot encoded\n    gt_class_indices = np.argmax(mask, axis=0) if mask.ndim == 3 else mask\n    gt_colored = colour_code_segmentation(gt_class_indices, select_class_rgb_values)\n\n    # Visualize\n    visualize(\n        original_image=original_image,  # Original image (not preprocessed)\n        preprocessed_image=image.transpose(1, 2, 0).astype('uint8'),  # Preprocessed image\n        ground_truth_mask=gt_colored,\n        predicted_mask=pred_colored\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:36:17.058452Z","iopub.execute_input":"2025-04-10T22:36:17.058747Z","iopub.status.idle":"2025-04-10T22:36:21.929522Z","shell.execute_reply.started":"2025-04-10T22:36:17.058724Z","shell.execute_reply":"2025-04-10T22:36:21.928523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict & Visualize predicted masks for original images without using the ground truth\nfor idx in sample_indices:\n    # Get preprocessed image\n    image = train_dataset[idx][0]  # Get image only (preprocessed)\n\n    # Load the original image (not preprocessed) for comparison\n    original_image_path = train_df.iloc[idx]['sat_image_path']  # Get the original image path\n    original_image = cv2.cvtColor(cv2.imread(original_image_path), cv2.COLOR_BGR2RGB)  # Load original image\n\n    # Convert preprocessed image to tensor\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n\n    # Predict mask\n    with torch.no_grad():\n        pred_logits = model(x_tensor)\n    pred_np = pred_logits.squeeze().cpu().numpy()\n\n    # Convert to class indices and color-coded mask\n    pred_class_indices = np.argmax(pred_np, axis=0)\n    pred_colored = colour_code_segmentation(pred_class_indices, select_class_rgb_values)\n\n    # Visualize\n    visualize(\n        original_image=original_image,  # Original image (not preprocessed)\n        preprocessed_image=image.transpose(1, 2, 0).astype('uint8'),  # Preprocessed image\n        predicted_mask=pred_colored  # Predicted mask\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:36:26.919877Z","iopub.execute_input":"2025-04-10T22:36:26.920160Z","iopub.status.idle":"2025-04-10T22:36:31.156988Z","shell.execute_reply.started":"2025-04-10T22:36:26.920138Z","shell.execute_reply":"2025-04-10T22:36:31.156005Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict & Visualize the comparison of predicted masks during training and testing\nfor idx in sample_indices:\n    # Get training image and ground truth mask\n    train_image, train_mask = train_dataset[idx]\n    original_image_path = train_df.iloc[idx]['sat_image_path']  # Get the original image path\n    original_image = cv2.cvtColor(cv2.imread(original_image_path), cv2.COLOR_BGR2RGB)  # Load original image\n\n    x_tensor = torch.from_numpy(train_image).to(DEVICE).unsqueeze(0)  # Convert preprocessed image to tensor\n\n    # Predict mask using the best model\n    with torch.no_grad():\n        pred_logits = best_model(x_tensor)\n    pred_np = pred_logits.squeeze().cpu().numpy()\n\n    # Convert to class indices and color-coded mask\n    pred_class_indices = np.argmax(pred_np, axis=0)\n    pred_colored = colour_code_segmentation(pred_class_indices, select_class_rgb_values)\n\n    # Convert ground truth mask (from training) to class indices and color-coded mask\n    train_mask_class_indices = np.argmax(train_mask, axis=0)  # Convert one-hot to class indices\n    train_mask_colored = colour_code_segmentation(train_mask_class_indices, select_class_rgb_values)\n\n    # Visualize the comparison\n    visualize(\n        original_image=original_image,  # Original image (not preprocessed)\n        preprocessed_image=train_image.transpose(1, 2, 0).astype('uint8'),  # Preprocessed image\n        predicted_mask_fr_training=train_mask_colored,  # Visualize the mask from training\n        predicted_mask_fr_best_model=pred_colored  # Predicted mask using the best model\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:36:36.123532Z","iopub.execute_input":"2025-04-10T22:36:36.123839Z","iopub.status.idle":"2025-04-10T22:36:40.777966Z","shell.execute_reply.started":"2025-04-10T22:36:36.123816Z","shell.execute_reply":"2025-04-10T22:36:40.777053Z"}},"outputs":[],"execution_count":null}]}