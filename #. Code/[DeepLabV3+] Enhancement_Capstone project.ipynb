{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1634186,"sourceType":"datasetVersion","datasetId":966140}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\n### In this notebook we use [DeepLabV3+](https://arxiv.org/abs/1802.02611) for Road Extraction from Satellite Imagery using [DeepGlobe Road Extraction Dataset](https://www.kaggle.com/balraj98/deepglobe-road-extraction-dataset).","metadata":{}},{"cell_type":"markdown","source":"### Libraries üìö‚¨á","metadata":{}},{"cell_type":"code","source":"import os, cv2\nimport numpy as np\nimport pandas as pd\nimport random, tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport albumentations as album","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:52:19.998150Z","iopub.execute_input":"2025-03-13T16:52:19.998422Z","iopub.status.idle":"2025-03-13T16:52:26.800518Z","shell.execute_reply.started":"2025-03-13T16:52:19.998395Z","shell.execute_reply":"2025-03-13T16:52:26.799825Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade pip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:52:31.259902Z","iopub.execute_input":"2025-03-13T16:52:31.260204Z","iopub.status.idle":"2025-03-13T16:52:37.171162Z","shell.execute_reply.started":"2025-03-13T16:52:31.260182Z","shell.execute_reply":"2025-03-13T16:52:37.170160Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\nDownloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.0.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\nimport segmentation_models_pytorch as smp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:52:39.461558Z","iopub.execute_input":"2025-03-13T16:52:39.461933Z","iopub.status.idle":"2025-03-13T16:52:54.402750Z","shell.execute_reply.started":"2025-03-13T16:52:39.461903Z","shell.execute_reply":"2025-03-13T16:52:54.401807Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Read Data & Create train / valid splits üìÅ","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '../input/deepglobe-road-extraction-dataset'\n\nmetadata_df = pd.read_csv(os.path.join(DATA_DIR, 'metadata.csv'))\nmetadata_df = metadata_df[metadata_df['split']=='train']\nmetadata_df = metadata_df[['image_id', 'sat_image_path', 'mask_path']]\nmetadata_df['sat_image_path'] = metadata_df['sat_image_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\nmetadata_df['mask_path'] = metadata_df['mask_path'].apply(lambda img_pth: os.path.join(DATA_DIR, img_pth))\n# Shuffle DataFrame\nmetadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n\n# Perform 90/10 split for train / val\nvalid_df = metadata_df.sample(frac=0.1, random_state=42)\ntrain_df = metadata_df.drop(valid_df.index)\nlen(train_df), len(valid_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:53:04.870597Z","iopub.execute_input":"2025-03-13T16:53:04.870943Z","iopub.status.idle":"2025-03-13T16:53:04.913000Z","shell.execute_reply.started":"2025-03-13T16:53:04.870920Z","shell.execute_reply":"2025-03-13T16:53:04.912148Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(5603, 623)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"metadata_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:53:17.256271Z","iopub.execute_input":"2025-03-13T16:53:17.256586Z","iopub.status.idle":"2025-03-13T16:53:17.276021Z","shell.execute_reply.started":"2025-03-13T16:53:17.256562Z","shell.execute_reply":"2025-03-13T16:53:17.275194Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   image_id                                     sat_image_path  \\\n0    272122  ../input/deepglobe-road-extraction-dataset/tra...   \n1    622973  ../input/deepglobe-road-extraction-dataset/tra...   \n2    207813  ../input/deepglobe-road-extraction-dataset/tra...   \n3    493904  ../input/deepglobe-road-extraction-dataset/tra...   \n4    110236  ../input/deepglobe-road-extraction-dataset/tra...   \n5    998129  ../input/deepglobe-road-extraction-dataset/tra...   \n6    682308  ../input/deepglobe-road-extraction-dataset/tra...   \n7    507376  ../input/deepglobe-road-extraction-dataset/tra...   \n8    664839  ../input/deepglobe-road-extraction-dataset/tra...   \n9    841576  ../input/deepglobe-road-extraction-dataset/tra...   \n\n                                           mask_path  \n0  ../input/deepglobe-road-extraction-dataset/tra...  \n1  ../input/deepglobe-road-extraction-dataset/tra...  \n2  ../input/deepglobe-road-extraction-dataset/tra...  \n3  ../input/deepglobe-road-extraction-dataset/tra...  \n4  ../input/deepglobe-road-extraction-dataset/tra...  \n5  ../input/deepglobe-road-extraction-dataset/tra...  \n6  ../input/deepglobe-road-extraction-dataset/tra...  \n7  ../input/deepglobe-road-extraction-dataset/tra...  \n8  ../input/deepglobe-road-extraction-dataset/tra...  \n9  ../input/deepglobe-road-extraction-dataset/tra...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>sat_image_path</th>\n      <th>mask_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>272122</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>622973</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207813</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>493904</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>110236</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>998129</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>682308</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>507376</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>664839</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>841576</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n      <td>../input/deepglobe-road-extraction-dataset/tra...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Define data augmentation\ndef get_training_augmentation():\n    train_transform = [\n        album.HorizontalFlip(p=0.5),\n        album.VerticalFlip(p=0.5),\n        album.ShiftScaleRotate(scale_limit=0.5, rotate_limit=45, shift_limit=0.1, p=1, border_mode=0),\n        album.RandomBrightnessContrast(p=0.2),\n        album.RandomGamma(p=0.2),\n        album.RandomCrop(height=256, width=256, p=1.0)\n    ]\n    return album.Compose(train_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:53:26.553782Z","iopub.execute_input":"2025-03-13T16:53:26.554106Z","iopub.status.idle":"2025-03-13T16:53:26.558958Z","shell.execute_reply.started":"2025-03-13T16:53:26.554083Z","shell.execute_reply":"2025-03-13T16:53:26.557921Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define normalization transformation\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn=None):\n    _transform = []\n    if preprocessing_fn:\n        _transform.append(album.Lambda(image=preprocessing_fn))\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n    return album.Compose(_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:53:32.304477Z","iopub.execute_input":"2025-03-13T16:53:32.304957Z","iopub.status.idle":"2025-03-13T16:53:32.309971Z","shell.execute_reply.started":"2025-03-13T16:53:32.304919Z","shell.execute_reply":"2025-03-13T16:53:32.309069Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\n# Custom dataset class\nclass RoadsDataset(Dataset):\n    def __init__(self, df, class_rgb_values=None, augmentation=None, preprocessing=None, target_size=(1024, 1024)):\n        self.image_paths = df['sat_image_path'].tolist()\n        self.mask_paths = df['mask_path'].tolist()\n        self.class_rgb_values = class_rgb_values\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.target_size = target_size\n    \n    def __getitem__(self, i):\n        # Read images and masks\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n        \n        # One-hot-encode the mask\n        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n        \n        # Apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        image = cv2.resize(image, self.target_size)\n        mask = cv2.resize(mask, self.target_size)\n        \n        # Apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        return image, mask\n    \n    def __len__(self):\n        return len(self.image_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:53:37.440021Z","iopub.execute_input":"2025-03-13T16:53:37.440418Z","iopub.status.idle":"2025-03-13T16:53:37.450532Z","shell.execute_reply.started":"2025-03-13T16:53:37.440385Z","shell.execute_reply":"2025-03-13T16:53:37.449606Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Helper functions for one-hot encoding and color coding\ndef one_hot_encode(label, label_values):\n    semantic_map = []\n    for colour in label_values:\n        equality = np.equal(label, colour)\n        class_map = np.all(equality, axis=-1)\n        semantic_map.append(class_map)\n    semantic_map = np.stack(semantic_map, axis=-1)\n    return semantic_map\n\ndef reverse_one_hot(image):\n    x = np.argmax(image, axis=-1)\n    return x\n\ndef colour_code_segmentation(image, label_values):\n    colour_codes = np.array(label_values)\n    x = colour_codes[image.astype(int)]\n    return x\n\n# Define class RGB values\nclass_dict = pd.read_csv(os.path.join(DATA_DIR, 'class_dict.csv'))\nclass_names = class_dict['name'].tolist()\nclass_rgb_values = class_dict[['r','g','b']].values.tolist()\n\n# Shortlist specific classes\nselect_classes = ['background', 'road']\nselect_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\nselect_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n\n# Get train and val dataset instances\ntrain_dataset = RoadsDataset(\n    train_df, \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(smp.encoders.get_preprocessing_fn('resnet50', 'imagenet')),\n    class_rgb_values=select_class_rgb_values,\n)\n\nvalid_dataset = RoadsDataset(\n    valid_df, \n    preprocessing=get_preprocessing(smp.encoders.get_preprocessing_fn('resnet50', 'imagenet')),\n    class_rgb_values=select_class_rgb_values,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:53:42.663731Z","iopub.execute_input":"2025-03-13T16:53:42.664038Z","iopub.status.idle":"2025-03-13T16:53:42.691629Z","shell.execute_reply.started":"2025-03-13T16:53:42.664016Z","shell.execute_reply":"2025-03-13T16:53:42.690967Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create dataloaders with reduced batch size\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False)\n\n# Initialize the DeepLabV3+ model\nmodel = smp.DeepLabV3Plus(\n    encoder_name=\"resnet50\",        # Use a more powerful encoder\n    encoder_weights=\"imagenet\",     # Use `imagenet` pre-trained weights for encoder initialization\n    in_channels=3,                  # Model input channels (1 for grayscale, 3 for RGB)\n    classes=len(select_classes),    # Model output channels (number of classes in your dataset)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:53:48.635330Z","iopub.execute_input":"2025-03-13T16:53:48.635622Z","iopub.status.idle":"2025-03-13T16:53:49.933852Z","shell.execute_reply.started":"2025-03-13T16:53:48.635598Z","shell.execute_reply":"2025-03-13T16:53:49.933112Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 327MB/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Define the learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n\n# Define combined loss function\nclass DiceBCELoss(nn.Module):\n    def __init__(self, smooth=1):\n        super(DiceBCELoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, y_true, y_pred):\n        y_true = y_true.contiguous()\n        y_pred = y_pred.contiguous()\n        \n        intersection = (y_true * y_pred).sum(dim=2).sum(dim=2)\n        dice_loss = (2. * intersection + self.smooth) / (y_true.sum(dim=2).sum(dim=2) + y_pred.sum(dim=2).sum(dim=2) + self.smooth)\n        dice_loss = 1 - dice_loss.mean()\n        \n        bce_loss = nn.functional.binary_cross_entropy_with_logits(y_pred, y_true)\n        loss = bce_loss + dice_loss\n        \n        return loss\n\n# Define the loss function\nloss_fn = DiceBCELoss()\n\n# Define IoU calculation\ndef calculate_iou(y_true, y_pred, smooth=1e-6):\n    y_true = y_true.contiguous()\n    y_pred = y_pred.contiguous()\n    \n    intersection = (y_true * y_pred).sum(dim=2).sum(dim=2)\n    union = y_true.sum(dim=2).sum(dim=2) + y_pred.sum(dim=2).sum(dim=2) - intersection\n    \n    iou = (intersection + smooth) / (union + smooth)\n    return iou.mean()\n\n# Define Early Stopping\nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:54:01.711522Z","iopub.execute_input":"2025-03-13T16:54:01.711867Z","iopub.status.idle":"2025-03-13T16:54:01.722782Z","shell.execute_reply.started":"2025-03-13T16:54:01.711840Z","shell.execute_reply":"2025-03-13T16:54:01.721982Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Training loop with mixed precision\nnum_epochs = 4\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nscaler = torch.cuda.amp.GradScaler()\nearly_stopping = EarlyStopping(patience=10, verbose=True)\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for images, masks in train_loader:\n        images = images.float().to(device)\n        masks = masks.float().to(device)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            loss = loss_fn(outputs, masks)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        train_loss += loss.item()\n    train_loss /= len(train_loader)\n\n    model.eval()\n    val_loss = 0\n    val_iou = 0\n    with torch.no_grad():\n        for images, masks in val_loader:\n            images = images.float().to(device)\n            masks = masks.float().to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                loss = loss_fn(outputs, masks)\n            val_loss += loss.item()\n            val_iou += calculate_iou(masks, outputs).item()\n    val_loss /= len(val_loader)\n    val_iou /= len(val_loader)\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}')\n\n    scheduler.step(val_iou)\n    early_stopping(val_loss, model)\n\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:54:07.513897Z","iopub.execute_input":"2025-03-13T16:54:07.514202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model\ntorch.save(model.state_dict(), 'best_model.pth')\n\n# Assuming you have stored the training and validation losses in lists\ntrain_losses = []  # Replace with your actual training loss values\nval_losses = []    # Replace with your actual validation loss values\n\nplt.figure(figsize=(20,8))\nplt.plot(range(num_epochs), train_losses, lw=3, label='Train')\nplt.plot(range(num_epochs), val_losses, lw=3, label='Valid')\nplt.xlabel('Epochs', fontsize=20)\nplt.ylabel('Loss', fontsize=20)\nplt.title('Training and Validation Loss', fontsize=20)\nplt.legend(loc='best', fontsize=16)\nplt.grid()\nplt.savefig('loss_plot.png')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-13T16:51:42.311Z"}},"outputs":[],"execution_count":null}]}